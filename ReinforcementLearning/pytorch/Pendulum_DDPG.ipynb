{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(-8.0, 8.0, (3,), float32)\n",
      "Box(-2.0, 2.0, (1,), float32)\n",
      "n_features: 3 n_actions: 1 b_actions: 2.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "env = gym.make('Pendulum-v0')\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "\n",
    "n_features = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.shape[0]\n",
    "b_actions = env.action_space.high[0]\n",
    "n_hiddens = 30\n",
    "\n",
    "print(\"n_features:\", n_features, \"n_actions:\", n_actions, \"b_actions:\", b_actions)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, n_features, n_hiddens, n_actions, b_actions):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.n_hiddens = n_hiddens\n",
    "        self.n_actions = n_actions\n",
    "        self.b_actions = b_actions\n",
    "\n",
    "        self.fc1 = nn.Linear(n_features, n_hiddens)\n",
    "        self.fc2 = nn.Linear(n_hiddens, n_actions)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.shape[-1] == self.n_features\n",
    "\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.tanh(self.fc2(x))\n",
    "        x = x * self.b_actions\n",
    "        return x\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, n_features, n_actions, n_hiddens):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.n_actions = n_actions\n",
    "        self.n_hiddens = n_hiddens\n",
    "\n",
    "        self.fc_s = nn.Linear(n_features, n_hiddens)\n",
    "        self.fc_a = nn.Linear(n_actions, n_hiddens)\n",
    "        self.fc = nn.Linear(n_hiddens, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, s, a):\n",
    "        assert s.shape[-1] == self.n_features\n",
    "        assert a.shape[-1] == self.n_actions\n",
    "        \n",
    "        s = self.fc_s(s)\n",
    "        a = self.fc_a(a)\n",
    "        x = self.fc(self.relu(s + a))\n",
    "        return x\n",
    "        \n",
    "class DDPGNetwork: \n",
    "    def __init__(self, n_features, n_actions, b_actions, n_hiddens):\n",
    "        self.n_features = n_features\n",
    "        self.n_actions = n_actions\n",
    "        self.b_actions = b_actions\n",
    "        self.n_hiddens = n_hiddens\n",
    "        self.pointer = 0\n",
    "        self.capacity = 10000\n",
    "        self.memory = np.zeros((self.capacity, n_features * 2 + n_actions + 1), dtype=np.float32)\n",
    "        self.tau = 0.02\n",
    "        \n",
    "        self.actor = Actor(n_features, n_hiddens, n_actions, b_actions)\n",
    "        self.actor_target = Actor(n_features, n_hiddens, n_actions, b_actions)\n",
    "        self.critic = Critic(n_features, n_actions, n_hiddens)\n",
    "        self.critic_target = Critic(n_features, n_actions, n_hiddens)\n",
    "        \n",
    "        self.actor_op = torch.optim.Adam(self.actor.parameters(), lr=0.001)\n",
    "        self.critic_op = torch.optim.Adam(self.critic.parameters(), lr=0.001)\n",
    "        \n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        a = self.actor(torch.Tensor(s)).detach().numpy()\n",
    "        return a\n",
    "    \n",
    "    def save_memory(self, s, a, r, s_):\n",
    "        self.memory[self.pointer % self.capacity] = np.hstack([s, a, [r], s_])\n",
    "        self.pointer += 1\n",
    "    \n",
    "    def learn(self, batch_size):\n",
    "        if self.pointer > self.capacity:\n",
    "            batch_index = np.random.choice(self.capacity, size=batch_size)\n",
    "        else:\n",
    "            batch_index = np.random.choice(self.pointer, size=batch_size)\n",
    "        \n",
    "        s = torch.from_numpy(self.memory[batch_index, :self.n_features])\n",
    "        a = torch.from_numpy(self.memory[batch_index, self.n_features:self.n_features + self.n_actions])\n",
    "        r = torch.from_numpy(self.memory[batch_index, -self.n_features-1:-self.n_features])\n",
    "        s_ = torch.from_numpy(self.memory[batch_index, -self.n_features:])\n",
    "        \n",
    "        q = self.critic(s, a)\n",
    "        a_ = self.actor_target(s_)\n",
    "        q_ = self.critic_target(s_, a_)\n",
    "        q_ = r + 0.9 * q_\n",
    "        critic_loss = self.mse(q_, q)\n",
    "        self.critic_op.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_op.step()\n",
    "        \n",
    "        actor_loss = -torch.mean(self.critic(s, self.actor(s)))\n",
    "        self.actor_op.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_op.step()\n",
    "        \n",
    "        self.update_net(self.tau)\n",
    "        \n",
    "    def update_net(self, tau):\n",
    "        for param, param_target in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "            param_target.data.copy_(tau * param.data + (1 - tau) * param_target.data)\n",
    "        for param, param_target in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "            param_target.data.copy_(tau * param.data + (1 - tau) * param_target.data)\n",
    "        \n",
    "    def save(self, actor_path, critic_path):\n",
    "        torch.save(self.actor, actor_path)\n",
    "        torch.save(self.critic, critic_path)\n",
    "    \n",
    "    def load(self, actor_path, critic_path):\n",
    "        self.actor = torch.load(actor_path)\n",
    "        self.critic = torch.load(critic_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: total reward -1033.\n",
      "epoch 1: total reward -1448.\n",
      "epoch 2: total reward -1285.\n",
      "epoch 3: total reward -1272.\n",
      "epoch 4: total reward -1029.\n",
      "epoch 5: total reward -1014.\n",
      "epoch 6: total reward -1293.\n",
      "epoch 7: total reward -1185.\n",
      "epoch 8: total reward -1372.\n",
      "epoch 9: total reward -1537.\n",
      "epoch 10: total reward -1625.\n",
      "epoch 11: total reward -1502.\n",
      "epoch 12: total reward -1012.\n",
      "epoch 13: total reward -1707.\n",
      "epoch 14: total reward -1648.\n",
      "epoch 15: total reward -1751.\n",
      "epoch 16: total reward -1553.\n",
      "epoch 17: total reward -1494.\n",
      "epoch 18: total reward -1288.\n",
      "epoch 19: total reward -1647.\n",
      "epoch 20: total reward -1150.\n",
      "epoch 21: total reward -1650.\n",
      "epoch 22: total reward -1309.\n",
      "epoch 23: total reward -1441.\n",
      "epoch 24: total reward -1478.\n",
      "epoch 25: total reward -1089.\n",
      "epoch 26: total reward -1447.\n",
      "epoch 27: total reward -993.\n",
      "epoch 28: total reward -1697.\n",
      "epoch 29: total reward -1671.\n",
      "epoch 30: total reward -1577.\n",
      "epoch 31: total reward -1138.\n",
      "epoch 32: total reward -1440.\n",
      "epoch 33: total reward -1472.\n",
      "epoch 34: total reward -1235.\n",
      "epoch 35: total reward -1290.\n",
      "epoch 36: total reward -1516.\n",
      "epoch 37: total reward -1008.\n",
      "epoch 38: total reward -1560.\n",
      "epoch 39: total reward -1026.\n",
      "epoch 40: total reward -1446.\n",
      "epoch 41: total reward -1328.\n",
      "epoch 42: total reward -1208.\n",
      "epoch 43: total reward -1300.\n",
      "epoch 44: total reward -1272.\n",
      "epoch 45: total reward -1618.\n",
      "epoch 46: total reward -1285.\n",
      "epoch 47: total reward -1183.\n",
      "epoch 48: total reward -1234.\n",
      "epoch 49: total reward -1626.\n",
      "epoch 50: total reward -1397.\n",
      "epoch 51: total reward -1531.\n",
      "epoch 52: total reward -1544.\n",
      "epoch 53: total reward -1585.\n",
      "epoch 54: total reward -1526.\n",
      "epoch 55: total reward -1552.\n",
      "epoch 56: total reward -1276.\n",
      "epoch 57: total reward -1393.\n",
      "epoch 58: total reward -1460.\n",
      "epoch 59: total reward -1516.\n",
      "epoch 60: total reward -1509.\n",
      "epoch 61: total reward -1440.\n",
      "epoch 62: total reward -1065.\n",
      "epoch 63: total reward -1299.\n",
      "epoch 64: total reward -1076.\n",
      "epoch 65: total reward -935.\n",
      "epoch 66: total reward -957.\n",
      "epoch 67: total reward -791.\n",
      "epoch 68: total reward -855.\n",
      "epoch 69: total reward -538.\n",
      "epoch 70: total reward -652.\n",
      "epoch 71: total reward -722.\n",
      "epoch 72: total reward -522.\n",
      "epoch 73: total reward -395.\n",
      "epoch 74: total reward -276.\n",
      "epoch 75: total reward -272.\n",
      "epoch 76: total reward -477.\n",
      "epoch 77: total reward -10.\n",
      "epoch 78: total reward -129.\n",
      "epoch 79: total reward -123.\n",
      "epoch 80: total reward -6.\n",
      "epoch 81: total reward -258.\n",
      "epoch 82: total reward -129.\n",
      "epoch 83: total reward -444.\n",
      "epoch 84: total reward -366.\n",
      "epoch 85: total reward -3.\n",
      "epoch 86: total reward -128.\n",
      "epoch 87: total reward -366.\n",
      "epoch 88: total reward -254.\n",
      "epoch 89: total reward -254.\n",
      "epoch 90: total reward -124.\n",
      "epoch 91: total reward -127.\n",
      "epoch 92: total reward -242.\n",
      "epoch 93: total reward -396.\n",
      "epoch 94: total reward -258.\n",
      "epoch 95: total reward -262.\n",
      "epoch 96: total reward -127.\n",
      "epoch 97: total reward -129.\n",
      "epoch 98: total reward -130.\n",
      "epoch 99: total reward -244.\n",
      "epoch 100: total reward -251.\n",
      "epoch 101: total reward -414.\n",
      "epoch 102: total reward -497.\n",
      "epoch 103: total reward -358.\n",
      "epoch 104: total reward -424.\n",
      "epoch 105: total reward -131.\n",
      "epoch 106: total reward -131.\n",
      "epoch 107: total reward -138.\n",
      "epoch 108: total reward -253.\n",
      "epoch 109: total reward -129.\n",
      "epoch 110: total reward -260.\n",
      "epoch 111: total reward -130.\n",
      "epoch 112: total reward -260.\n",
      "epoch 113: total reward -255.\n",
      "epoch 114: total reward -381.\n",
      "epoch 115: total reward -387.\n",
      "epoch 116: total reward -127.\n",
      "epoch 117: total reward -132.\n",
      "epoch 118: total reward -129.\n",
      "epoch 119: total reward -134.\n",
      "epoch 120: total reward -131.\n",
      "epoch 121: total reward -800.\n",
      "epoch 122: total reward -1494.\n",
      "epoch 123: total reward -9.\n",
      "epoch 124: total reward -518.\n",
      "epoch 125: total reward -11.\n",
      "epoch 126: total reward -810.\n",
      "epoch 127: total reward -6.\n",
      "epoch 128: total reward -912.\n",
      "epoch 129: total reward -902.\n",
      "epoch 130: total reward -785.\n",
      "epoch 131: total reward -948.\n",
      "epoch 132: total reward -816.\n",
      "epoch 133: total reward -136.\n",
      "epoch 134: total reward -402.\n",
      "epoch 135: total reward -134.\n",
      "epoch 136: total reward -135.\n",
      "epoch 137: total reward -133.\n",
      "epoch 138: total reward -130.\n",
      "epoch 139: total reward -137.\n",
      "epoch 140: total reward -13.\n",
      "epoch 141: total reward -273.\n",
      "epoch 142: total reward -1.\n",
      "epoch 143: total reward -247.\n",
      "epoch 144: total reward -144.\n",
      "epoch 145: total reward -386.\n",
      "epoch 146: total reward -241.\n",
      "epoch 147: total reward -257.\n",
      "epoch 148: total reward -124.\n",
      "epoch 149: total reward -269.\n",
      "epoch 150: total reward -135.\n",
      "epoch 151: total reward -140.\n",
      "epoch 152: total reward -279.\n",
      "epoch 153: total reward -138.\n",
      "epoch 154: total reward -262.\n",
      "epoch 155: total reward -138.\n",
      "epoch 156: total reward -8.\n",
      "epoch 157: total reward -138.\n",
      "epoch 158: total reward -8.\n",
      "epoch 159: total reward -135.\n",
      "epoch 160: total reward -137.\n",
      "epoch 161: total reward -268.\n",
      "epoch 162: total reward -138.\n",
      "epoch 163: total reward -135.\n",
      "epoch 164: total reward -135.\n",
      "epoch 165: total reward -276.\n",
      "epoch 166: total reward -144.\n",
      "epoch 167: total reward -129.\n",
      "epoch 168: total reward -276.\n",
      "epoch 169: total reward -272.\n",
      "epoch 170: total reward -137.\n",
      "epoch 171: total reward -257.\n",
      "epoch 172: total reward -132.\n",
      "epoch 173: total reward -130.\n",
      "epoch 174: total reward -341.\n",
      "epoch 175: total reward -266.\n",
      "epoch 176: total reward -131.\n",
      "epoch 177: total reward -126.\n",
      "epoch 178: total reward -406.\n",
      "epoch 179: total reward -127.\n",
      "epoch 180: total reward -244.\n",
      "epoch 181: total reward -277.\n",
      "epoch 182: total reward -3.\n",
      "epoch 183: total reward -133.\n",
      "epoch 184: total reward -128.\n",
      "epoch 185: total reward -124.\n",
      "epoch 186: total reward -132.\n",
      "epoch 187: total reward -4.\n",
      "epoch 188: total reward -5.\n",
      "epoch 189: total reward -137.\n",
      "epoch 190: total reward -134.\n",
      "epoch 191: total reward -246.\n",
      "epoch 192: total reward -373.\n",
      "epoch 193: total reward -256.\n",
      "epoch 194: total reward -135.\n",
      "epoch 195: total reward -276.\n",
      "epoch 196: total reward -8.\n",
      "epoch 197: total reward -130.\n",
      "epoch 198: total reward -134.\n",
      "epoch 199: total reward -311.\n"
     ]
    }
   ],
   "source": [
    "ddpg = DDPGNetwork(n_features, n_actions, b_actions, n_hiddens)\n",
    "eps = 200\n",
    "var = 3\n",
    "for ep in range(eps):\n",
    "    s = env.reset()\n",
    "    ep_r = 0\n",
    "    while(True):\n",
    "        env.render()\n",
    "        a = ddpg.choose_action(s)\n",
    "        a = np.clip(np.random.normal(a, var), -2, 2)\n",
    "        s_, r, done, _ = env.step(a)\n",
    "        ep_r += r\n",
    "        \n",
    "        ddpg.save_memory(s, a, r/10, s_)\n",
    "        if ddpg.pointer >= ddpg.capacity:\n",
    "            var *= 0.9995\n",
    "            ddpg.learn(100)\n",
    "            \n",
    "        s = s_\n",
    "        \n",
    "        if done:\n",
    "            print('epoch %d: total reward %d.' % (ep, ep_r))\n",
    "            break\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg.save('model/DDPG_Actor.pkl', 'model/DDPG_Critic.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: total reward -263.\n",
      "epoch 1: total reward -129.\n",
      "epoch 2: total reward -131.\n",
      "epoch 3: total reward -125.\n",
      "epoch 4: total reward -129.\n",
      "epoch 5: total reward -390.\n",
      "epoch 6: total reward -128.\n",
      "epoch 7: total reward -129.\n",
      "epoch 8: total reward -5.\n",
      "epoch 9: total reward -132.\n"
     ]
    }
   ],
   "source": [
    "ddpg = DDPGNetwork(n_features, n_actions, b_actions, n_hiddens)\n",
    "ddpg.load('model/DDPG_Actor.pkl', 'model/DDPG_Critic.pkl')\n",
    "\n",
    "eps = 10\n",
    "for ep in range(eps):\n",
    "    s = env.reset()\n",
    "    ep_r = 0\n",
    "    while(True):\n",
    "        env.render()\n",
    "        a = ddpg.choose_action(s)\n",
    "        s, r, done, _ = env.step(a)\n",
    "        ep_r += r\n",
    "        \n",
    "        if done:\n",
    "            print('epoch %d: total reward %d.' % (ep, ep_r))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
