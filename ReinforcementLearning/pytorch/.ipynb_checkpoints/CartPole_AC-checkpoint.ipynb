{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
      "Discrete(2)\n",
      "obs:4, hidden:10, act:2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "\n",
    "n_features = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "n_hidden = 10\n",
    "print('obs:%d, hidden:%d, act:%d' % (n_features, n_hidden, n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNet:\n",
    "    def __init__(self, n_features, n_hidden, n_actions, learning_rate=0.001):\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_actions = n_actions\n",
    "        self.net = self._build_net(n_features, n_hidden, n_actions)\n",
    "        self.opt = torch.optim.Adam(self.net.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def _build_net(self, n_features, n_hidden, n_actions):\n",
    "        net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_features, n_hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(n_hidden, n_actions),\n",
    "            torch.nn.Softmax()\n",
    "        )\n",
    "        return net\n",
    "        \n",
    "    def choose_action(self, obs):\n",
    "        act_prob = self.net(torch.Tensor(obs))\n",
    "        action = torch.multinomial(act_prob, 1)[0]\n",
    "        return act_prob, action\n",
    "    \n",
    "    def learn(self, obs, a, td_error):\n",
    "        act_prob = self.net(torch.Tensor(obs))\n",
    "        log_prob = torch.log(act_prob[a])\n",
    "        loss = -log_prob * td_error\n",
    "        \n",
    "        self.opt.zero_grad()\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "    \n",
    "    def save(self, save_name):\n",
    "        torch.save(self.net, save_name)\n",
    "        \n",
    "    def load(self, load_name):\n",
    "        self.net = torch.load(load_name)\n",
    "        \n",
    "class CriticNet:\n",
    "    def __init__(self, n_features, n_hidden, learning_rate=0.01, gamma=0.95):\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.gamma = gamma\n",
    "        self.net = self._build_net(n_features, n_hidden)\n",
    "        self.opt = torch.optim.Adam(self.net.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def _build_net(self, n_features, n_hidden):\n",
    "        net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_features, n_hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(n_hidden, 1)\n",
    "        )\n",
    "        return net\n",
    "    \n",
    "    def learn(self, obs, reward, obs_):\n",
    "        val = self.net(torch.Tensor(obs))\n",
    "        val_ = self.net(torch.Tensor(obs_)).detach()\n",
    "        td_error = reward + self.gamma * val_ - val\n",
    "        loss = torch.square(td_error)\n",
    "        \n",
    "        self.opt.zero_grad()\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "        \n",
    "        return td_error.detach()\n",
    "    \n",
    "    def save(self, save_name):\n",
    "        torch.save(self.net, save_name)\n",
    "        \n",
    "    def load(self, load_name):\n",
    "        self.net = torch.load(load_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0  reward: 74  round: 16\n",
      "episode: 1  reward: 65  round: 11\n",
      "episode: 2  reward: 60  round: 34\n",
      "episode: 3  reward: 54  round: 22\n",
      "episode: 4  reward: 48  round: 13\n",
      "episode: 5  reward: 43  round: 18\n",
      "episode: 6  reward: 38  round: 21\n",
      "episode: 7  reward: 34  round: 12\n",
      "episode: 8  reward: 29  round: 12\n",
      "episode: 9  reward: 26  round: 14\n",
      "episode: 10  reward: 23  round: 21\n",
      "episode: 11  reward: 22  round: 34\n",
      "episode: 12  reward: 19  round: 17\n",
      "episode: 13  reward: 17  round: 21\n",
      "episode: 14  reward: 16  round: 24\n",
      "episode: 15  reward: 13  round: 13\n",
      "episode: 16  reward: 13  round: 34\n",
      "episode: 17  reward: 11  round: 12\n",
      "episode: 18  reward: 12  round: 44\n",
      "episode: 19  reward: 12  round: 34\n",
      "episode: 20  reward: 12  round: 30\n",
      "episode: 21  reward: 12  round: 30\n",
      "episode: 22  reward: 12  round: 42\n",
      "episode: 23  reward: 11  round: 20\n",
      "episode: 24  reward: 9  round: 11\n",
      "episode: 25  reward: 8  round: 17\n",
      "episode: 26  reward: 7  round: 22\n",
      "episode: 27  reward: 5  round: 13\n",
      "episode: 28  reward: 6  round: 30\n",
      "episode: 29  reward: 4  round: 12\n",
      "episode: 30  reward: 5  round: 33\n",
      "episode: 31  reward: 4  round: 15\n",
      "episode: 32  reward: 4  round: 28\n",
      "episode: 33  reward: 4  round: 22\n",
      "episode: 34  reward: 3  round: 18\n",
      "episode: 35  reward: 2  round: 13\n",
      "episode: 36  reward: 0  round: 10\n",
      "episode: 37  reward: 0  round: 13\n",
      "episode: 38  reward: 0  round: 16\n",
      "episode: 39  reward: 0  round: 22\n",
      "episode: 40  reward: 0  round: 26\n",
      "episode: 41  reward: 0  round: 26\n",
      "episode: 42  reward: 0  round: 20\n",
      "episode: 43  reward: 1  round: 33\n",
      "episode: 44  reward: 3  round: 38\n",
      "episode: 45  reward: 3  round: 23\n",
      "episode: 46  reward: 2  round: 19\n",
      "episode: 47  reward: 1  round: 17\n",
      "episode: 48  reward: 2  round: 29\n",
      "episode: 49  reward: 2  round: 26\n",
      "episode: 50  reward: 3  round: 33\n",
      "episode: 51  reward: 2  round: 10\n",
      "episode: 52  reward: 1  round: 18\n",
      "episode: 53  reward: 0  round: 14\n",
      "episode: 54  reward: 3  round: 44\n",
      "episode: 55  reward: 2  round: 18\n",
      "episode: 56  reward: 2  round: 21\n",
      "episode: 57  reward: 3  round: 34\n",
      "episode: 58  reward: 2  round: 17\n",
      "episode: 59  reward: 3  round: 30\n",
      "episode: 60  reward: 2  round: 18\n",
      "episode: 61  reward: 2  round: 24\n",
      "episode: 62  reward: 1  round: 12\n",
      "episode: 63  reward: 2  round: 34\n",
      "episode: 64  reward: 1  round: 13\n",
      "episode: 65  reward: 0  round: 11\n",
      "episode: 66  reward: 0  round: 22\n",
      "episode: 67  reward: 0  round: 13\n",
      "episode: 68  reward: 0  round: 18\n",
      "episode: 69  reward: -1  round: 13\n",
      "episode: 70  reward: 0  round: 32\n",
      "episode: 71  reward: 4  round: 64\n",
      "episode: 72  reward: 5  round: 35\n",
      "episode: 73  reward: 3  round: 10\n",
      "episode: 74  reward: 2  round: 17\n",
      "episode: 75  reward: 1  round: 10\n",
      "episode: 76  reward: 6  round: 69\n",
      "episode: 77  reward: 4  round: 14\n",
      "episode: 78  reward: 3  round: 14\n",
      "episode: 79  reward: 2  round: 17\n",
      "episode: 80  reward: 2  round: 23\n",
      "episode: 81  reward: 2  round: 26\n",
      "episode: 82  reward: 2  round: 19\n",
      "episode: 83  reward: 2  round: 25\n",
      "episode: 84  reward: 1  round: 13\n",
      "episode: 85  reward: 2  round: 34\n",
      "episode: 86  reward: 3  round: 28\n",
      "episode: 87  reward: 2  round: 15\n",
      "episode: 88  reward: 1  round: 17\n",
      "episode: 89  reward: 2  round: 29\n",
      "episode: 90  reward: 1  round: 20\n",
      "episode: 91  reward: 1  round: 18\n",
      "episode: 92  reward: 0  round: 14\n",
      "episode: 93  reward: 0  round: 15\n",
      "episode: 94  reward: 0  round: 20\n",
      "episode: 95  reward: 0  round: 15\n",
      "episode: 96  reward: 0  round: 19\n",
      "episode: 97  reward: 0  round: 31\n",
      "episode: 98  reward: 0  round: 14\n",
      "episode: 99  reward: 0  round: 16\n",
      "episode: 100  reward: 0  round: 36\n",
      "episode: 101  reward: 0  round: 22\n",
      "episode: 102  reward: 3  round: 53\n",
      "episode: 103  reward: 2  round: 15\n",
      "episode: 104  reward: 1  round: 13\n",
      "episode: 105  reward: 1  round: 24\n",
      "episode: 106  reward: 5  round: 59\n",
      "episode: 107  reward: 4  round: 13\n",
      "episode: 108  reward: 3  round: 21\n",
      "episode: 109  reward: 3  round: 26\n",
      "episode: 110  reward: 5  round: 38\n",
      "episode: 111  reward: 6  round: 37\n",
      "episode: 112  reward: 6  round: 33\n",
      "episode: 113  reward: 6  round: 24\n",
      "episode: 114  reward: 5  round: 17\n",
      "episode: 115  reward: 5  round: 24\n",
      "episode: 116  reward: 6  round: 36\n",
      "episode: 117  reward: 5  round: 25\n",
      "episode: 118  reward: 8  round: 48\n",
      "episode: 119  reward: 8  round: 35\n",
      "episode: 120  reward: 7  round: 17\n",
      "episode: 121  reward: 9  round: 52\n",
      "episode: 122  reward: 9  round: 26\n",
      "episode: 123  reward: 9  round: 35\n",
      "episode: 124  reward: 10  round: 38\n",
      "episode: 125  reward: 9  round: 25\n",
      "episode: 126  reward: 14  round: 78\n",
      "episode: 127  reward: 12  round: 15\n",
      "episode: 128  reward: 19  round: 101\n",
      "episode: 129  reward: 22  round: 74\n",
      "episode: 130  reward: 24  round: 58\n",
      "episode: 131  reward: 24  round: 52\n",
      "episode: 132  reward: 26  round: 68\n",
      "episode: 133  reward: 27  round: 50\n",
      "episode: 134  reward: 24  round: 26\n",
      "episode: 135  reward: 27  round: 76\n",
      "episode: 136  reward: 29  round: 62\n",
      "episode: 137  reward: 29  round: 50\n",
      "episode: 138  reward: 29  round: 50\n",
      "episode: 139  reward: 31  round: 70\n",
      "episode: 140  reward: 28  round: 28\n",
      "episode: 141  reward: 27  round: 35\n",
      "episode: 142  reward: 27  round: 47\n",
      "episode: 143  reward: 26  round: 46\n",
      "episode: 144  reward: 26  round: 44\n",
      "episode: 145  reward: 25  round: 36\n",
      "episode: 146  reward: 28  round: 80\n",
      "episode: 147  reward: 25  round: 17\n",
      "episode: 148  reward: 22  round: 19\n",
      "episode: 149  reward: 24  round: 58\n",
      "episode: 150  reward: 34  round: 153\n",
      "episode: 151  reward: 33  round: 46\n",
      "episode: 152  reward: 31  round: 27\n",
      "episode: 153  reward: 29  round: 35\n",
      "episode: 154  reward: 30  round: 59\n",
      "episode: 155  reward: 32  round: 78\n",
      "episode: 156  reward: 33  round: 62\n",
      "episode: 157  reward: 33  round: 53\n",
      "episode: 158  reward: 31  round: 31\n",
      "episode: 159  reward: 27  round: 19\n",
      "episode: 160  reward: 27  round: 48\n",
      "episode: 161  reward: 25  round: 22\n",
      "episode: 162  reward: 24  round: 38\n",
      "episode: 163  reward: 21  round: 16\n",
      "episode: 164  reward: 21  round: 47\n",
      "episode: 165  reward: 24  round: 65\n",
      "episode: 166  reward: 29  round: 99\n",
      "episode: 167  reward: 31  round: 75\n",
      "episode: 168  reward: 34  round: 81\n",
      "episode: 169  reward: 33  round: 45\n",
      "episode: 170  reward: 33  round: 54\n",
      "episode: 171  reward: 32  round: 47\n",
      "episode: 172  reward: 31  round: 37\n",
      "episode: 173  reward: 30  round: 49\n",
      "episode: 174  reward: 28  round: 32\n",
      "episode: 175  reward: 28  round: 43\n",
      "episode: 176  reward: 30  round: 76\n",
      "episode: 177  reward: 29  round: 43\n",
      "episode: 178  reward: 30  round: 57\n",
      "episode: 179  reward: 28  round: 26\n",
      "episode: 180  reward: 27  round: 40\n",
      "episode: 181  reward: 26  round: 47\n",
      "episode: 182  reward: 26  round: 45\n",
      "episode: 183  reward: 27  round: 51\n",
      "episode: 184  reward: 28  round: 65\n",
      "episode: 185  reward: 30  round: 71\n",
      "episode: 186  reward: 30  round: 48\n",
      "episode: 187  reward: 32  round: 67\n",
      "episode: 188  reward: 33  round: 67\n",
      "episode: 189  reward: 34  round: 70\n",
      "episode: 190  reward: 43  round: 146\n",
      "episode: 191  reward: 42  round: 55\n",
      "episode: 192  reward: 44  round: 82\n",
      "episode: 193  reward: 41  round: 33\n",
      "episode: 194  reward: 48  round: 137\n",
      "episode: 195  reward: 48  round: 62\n",
      "episode: 196  reward: 48  round: 77\n",
      "episode: 197  reward: 48  round: 69\n",
      "episode: 198  reward: 57  round: 157\n",
      "episode: 199  reward: 55  round: 61\n",
      "episode: 200  reward: 54  round: 60\n",
      "episode: 201  reward: 53  round: 66\n",
      "episode: 202  reward: 55  round: 93\n",
      "episode: 203  reward: 62  round: 147\n",
      "episode: 204  reward: 59  round: 60\n",
      "episode: 205  reward: 58  round: 67\n",
      "episode: 206  reward: 63  round: 129\n",
      "episode: 207  reward: 77  round: 200\n",
      "episode: 208  reward: 89  round: 200\n",
      "episode: 209  reward: 87  round: 88\n",
      "episode: 210  reward: 82  round: 59\n",
      "episode: 211  reward: 85  round: 137\n",
      "episode: 212  reward: 88  round: 131\n",
      "episode: 213  reward: 95  round: 186\n",
      "episode: 214  reward: 96  round: 127\n",
      "episode: 215  reward: 107  round: 200\n",
      "episode: 216  reward: 116  round: 200\n",
      "episode: 217  reward: 124  round: 200\n",
      "episode: 218  reward: 132  round: 200\n",
      "episode: 219  reward: 139  round: 200\n",
      "episode: 220  reward: 145  round: 200\n",
      "episode: 221  reward: 150  round: 200\n",
      "episode: 222  reward: 155  round: 200\n",
      "episode: 223  reward: 160  round: 200\n",
      "episode: 224  reward: 164  round: 200\n",
      "episode: 225  reward: 167  round: 200\n",
      "episode: 226  reward: 170  round: 200\n",
      "episode: 227  reward: 173  round: 200\n"
     ]
    }
   ],
   "source": [
    "actor = ActorNet(n_features, n_hidden, n_actions)\n",
    "critic = CriticNet(n_features, n_hidden)\n",
    "\n",
    "stop = False\n",
    "eps = 3000\n",
    "for ep in range(eps):\n",
    "    obs = env.reset()\n",
    "    i = 0\n",
    "    track_r = []\n",
    "    while True:\n",
    "        i += 1\n",
    "        env.render()\n",
    "        act_prob, action = actor.choose_action(obs)\n",
    "        obs_, reward, done, info = env.step(action.numpy())\n",
    "        if done and i < 200:\n",
    "            reward = -20\n",
    "        track_r.append(reward)\n",
    "        td_error = critic.learn(obs, reward, obs_)\n",
    "        actor.learn(obs, action, td_error)\n",
    "        \n",
    "        obs = obs_\n",
    "        \n",
    "        if done:\n",
    "            ep_rs_sum = np.sum(track_r)\n",
    "            if 'running_reward' not in globals():\n",
    "                running_reward = ep_rs_sum\n",
    "            else:\n",
    "                running_reward = running_reward * 0.9 + ep_rs_sum * 0.1\n",
    "            print('episode:', ep, ' reward:', int(running_reward), ' round:', i)\n",
    "            if int(running_reward) > 170:\n",
    "                stop = True\n",
    "            break\n",
    "    if stop:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "model = torch.load('./model/AC_Actor.pkl')\n",
    "\n",
    "for i in range(10):\n",
    "    step = 0\n",
    "    obs = env.reset()\n",
    "    while(True):\n",
    "        env.render()\n",
    "        act = torch.argmax(model(torch.Tensor(obs)))\n",
    "        obs, _, done, _ = env.step(act.numpy())\n",
    "        step += 1\n",
    "        if done:\n",
    "            print('Total steps:', step)\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
